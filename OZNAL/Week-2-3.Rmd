---
title: "Week_2-3"
output: html_document
date: "2024-03-15"
---

# Task for 3. week

```{r}
library(tidyverse)
library(magrittr) # Pipe implementation for R
library(data.table) # Implements %like%

#setwd("...") # Set the correct path to players_22.csv
data <- read_csv("Files/players_22.csv", col_names = TRUE, num_threads = 4) # Read data into R
data # Check that data are properly loaded 
```

```{r}
data %<>% mutate(
  role = case_when(
    .$player_positions %like% "CF|LW|RW|ST|CAM" ~ "Offense",
    .$player_positions %like% "LWB|RWB|LB|RB|CDM" ~ "Defense",
    .$player_positions %like% "GK" ~ "Goal Keeper",
    .default = "Midfielders"
  )
)
```

```{r}
data %<>% mutate(role_oneHot = ifelse(role == "Offense",1,0))
```

```{r}
model <- data %>%
  select(overall,preferred_foot,skill_dribbling,role_oneHot) %$%
  glm(role_oneHot ~ skill_dribbling + preferred_foot, data=., family = binomial) # Fill in your code here. 
summary(model)
```

## Task 1.

Consider a player classification system, where “offensive” is the positive class (1) and “defensive/midfielder” is the negative class (0). Extract one-hot-encoded classes from the `model` object or the original tibble `data` (hint: these data exist in both). You should get a numeric vector of 0s and 1s as shown below.

```{r}
real.classes = data$role_oneHot
head(real.classes)
```

```{r}
head(model$y)
```

## Task 2.

Extract class probabilities from the `model` object. Use a 50% cut-off value for player classification. Save the resulting vector as `predicted.classes`.

```{r}
predicted.class.probabilities <- model$fitted.values # Fill in your code here
predicted.classes <- ifelse(predicted.class.probabilities > 0.5,1,0) # Fill in your code here

map_df(list(Real.Classes = real.classes, 
            Predicted.Probabilities = predicted.class.probabilities, 
            Predicted.CLasses = predicted.classes), head)
```

## Task 3.

Cross-tabulate real and predicted classes. Use `confusionMatrix()` from the caret package. `confusionMatrix()` accepts `predicted.classes` and `real.classes` only as factors.

```{r}
library(caret)
confMat = confusionMatrix(as_factor(predicted.classes), as_factor(real.classes), positive = "1")
confMat
```

```{r}
table(predicted.classes, real.classes)
```

## Task 4.

Inspect your readout. Which of the above statistics have you computed in the last task of **EIA Lab 2.2**?

---\> About same, only difference is in counts of confusion matrix values, and that this one has about \~0.1 higher accuraci.

## Task 5.

Revisit the contingency matrix from **Task 3**. Fill in a R code that calculates accuracy using the following formula.

Accuracy = (TP + TN)/(P+N)

```{r}
for (i in confMat$table) {
  print(i)
}
```

```{r}
#View(confMat)
allN = length(real.classes)
tn = confMat$table[1]
tp = confMat$table[4]
p = length(real.classes[real.classes != 0])
n = length(real.classes[real.classes == 0])
Accuracy <- (tp + tn)/(p+n) # Fill in your code here
Accuracy
```

```{r}
valTable = table(real.classes,predicted.classes)
tn = valTable[1]
tp = valTable[4]
fn = valTable[2]
fp = valTable[3]
p = tp + fn
n = tn + fp
Accuracy2 <- (tp + tn)/(p+n) # Fill in your code here
Accuracy2
```

## Task 6.

You can now calculate the proportion of classes with positive and negative labels to see how much is your dataset imbalanced. Is it?

```{r}
proportion.of.1 <- (tp + fn) / length(real.classes) # Fill in your code here
proportion.of.0 <- (tn + fp) / length(real.classes) # Fill in your code here
paste("Class proportions are: ", proportion.of.1, proportion.of.0, "for 1 and 0, respectively." )
```

## Task 7.

Calculate precision after retrieving counts from the confusion matrix.

```{r}
precision <- tp /(tp + fp) # Fill in your code here
precision
```

## Task 8.

You have already calculated precision, so let’s calculate recall this time. Use information from the contingency table generated by caret.

```{r}
recall <- tp / p # Fill in your code here
recall
```

## Task 9.

 The caret-implemented `confusionMatrix()` function calculates a lot of statistics, but not F1 score. Your task is to calculate F1 score using the data provided by caret.

```{r}
f1_score = 2 * (precision*recall)/(precision+recall)
f1_score
```
