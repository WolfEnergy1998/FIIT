1 Introduction
Security evaluation labs are facing new challenges every day as the adversaries are becoming more powerful considering the resources available and more advanced in terms of methods and techniques they use. Thus, machine learning (ML) is becoming indispensable for secure cryptographic implementations and ML methods are becoming mandatory in security evaluations. This aspect of AI for physical attacks is elaborated in more detail in [58, 59].

We are also witnessing an increase in intellectual property (IP) preserving strategies for various industries such as media content protection (Netflix and Spotify), automotive, wearables (such as watches and wristbands) etc. Basically, for those cases when optimized neural networks are of commercial interest, model details are often kept undisclosed. There are many reasons for keeping the neural network architectures secret. Often, those pre-trained models might provide additional information regarding the training data, which can be very sensitive. One critical example, is that if the model was trained on medical records of patients [19], confidential information could be encoded into the network during the training phase. Another use case of a neural net as an IP is in products using deep learning for high definition maps and cameras e.g. in automotive applications. As those markets are very competitive; the more efficient and effective neural networks are, the more successful companies selling them become.

Hence, determining the layout of the network with e.g. trained weights is a desirable target for an attacker. There are several ways to do it. First, the attacker might try to train new models, providing he/she has the access to target neural nets and training data. Second, the attacker could reverse engineer the neural nets of interest by using some additional information that becomes available while the device under attack is operating. This additional information is often physical and provided as a side channel e.g. timing, electromagnetic emanation (EM), or as a result of an active manipulations such as through fault injection responses and similar.

Relevant previous works on reverse engineering neural nets via side channels showed a lot of promise for this kind of research [4, 5]. It was demonstrated that a side-channel attacker is capable of reverse engineering proprietary information from an ARM Cortex-M3 microcontroller, which is a platform often used in edge devices using neural networks such as wearables, surveillance cameras etc. Other works considering fault injection techniques and the information learned from this kind of attacks followed [6].

1.1 Machine Learning for Edge Devices
In this chapter, we are focusing on Edge devices that can be used in applications like automotive, security cameras, wristbands, smart factory etc. Edge devices are those that collect, process and store data, close to the things/edges/sensors where the information is produced and gathered. This paradigm on computing saves a lot on latency and enables real time processing, as compared to the cloud based alternatives. With time edge devices have been strengthened with enhanced capabilities like artificial intelligence (AI) to provide decision making power to them. This integration of AI or in particular machine learning capabilities to edge devices is popularly known as EdgeML [14]. These devices can range from general purpose hardware like an Arm CortexM3 microcontroller, or those with dedicated hardware support for ML processing like Nvidia Jetson, Intel neural compute stick etc.

1.2 Attacks on Machine Learning
Security and reliability of machine learning has been a rising matter of concern in recent years. Here we briefly discuss some of the most prominent types of attacks which have been reported.

Model extraction attacks were reported targeting machine learning algorithms. The main idea behind such attacks is to query a victim model as a black box, with chosen/known input data and try to construct another model which mimics the victim model in prediction and performance. Such attacks are widely popular in the Machine learning as a Service (MLaaS) paradigm, where an API of the victim model is available to the adversary on a pay per use basis [51] and the adversary aims to recover an equivalent model with minimum number of queries. Apart from model extraction, known attacks were extended to the recovery of training data by techniques like membership inference and model inversion. Shokri et al. [45] reported the leakage of sensitive information from machine learning models about individual data records used for training. They show that such models are vulnerable to membership inference attacks. Details about training data can also be leaked through model inversion as presented by Fredrikson et al. [18].

Other kinds of attacks compromising the reliability of machine learning were reported. These attacks systematically lead a trained model to predict an incorrect output, resulting in a so-called an evasion attack [48]. The modus operandi of this attack involves small changes in the input that push the model to cross the decision boundaries during the classification. This effectively changes the resulting output class. The perturbed inputs were later called “adversarial examples” and can be used for various different purposes, including targeted and untargeted misclassification, denial of service, etc.

All such attacks target the model behavior and run independent of implementation style. Normally, such an API model would be hosted on the cloud. As the attacks appeared, appropriate mitigation were also proposed, for example, against model extraction attacks [30, 35], evasion [11] and adversarial attacks [12].

With the adoption of machine learning for edge devices under EdgeML paradigm, new vulnerabilities arose. The computation of resource intensive machine learning algorithms leaves a non-negligible physical signature for every execution. This physical signature depends on the model parameters and inputs. If the adversary who controls the inputs can measure this physical signature, it is possible to learn information about otherwise black box model. The physical signature can be in form of timing, power consumption, cache access patterns etc. A survey of side-channel based attacks on machine learning discussion on mitigation techniques is reported in Sect. 2.

EdgeML also exposes the machine learning computation to active adversary which are capable of disturbing the computation through intentional perturbations. The introduced disturbances to the computation may often lead the trained model to predict an incorrect output. From the higher level, these attack are similar to evasion attacks with adversarial attacks. However, an advanced adversary capable of fault injection can be much more powerful as the perturbations can potentially be inserted at an arbitrary point of the computation, giving a more precise control to the adversary. Further modification of these attacks can also lead to model extraction or denial of service. A survey of fault-injection based attacks on machine learning and discussion on mitigation techniques is reported in Sect. 3.

2 Overview Side Channel Threats to Machine Learning
With the systematic deployment of ML models on edge devices, unprecedented physical access to those models has lead to new security vulnerabilities. While classically these ML models are considered a black box, physical access permits different adversaries to snoop direct or indirect information about the internal execution. In this section, we provide an overview of recent works which used side-channel analysis to compromise ML models.

Side-channel attacks (SCA) are passive attacks that observe physical quantities related to computation of sensitive variables (dependent on secret data) and exploit it to gain confidential information. SCA have been widely studied in the community of information security and cryptography over the past two decades to demonstrate vulnerabilities in implementations of various security and cryptographic algorithms including both symmetric and asymmetric key primitives. They exploit physical traits like power consumption, computation time, cache access patterns, electromagnetic (EM) emanation, etc. Typical attacks relying on statistical methods are Differential Power Analysis (DPA) or Differential ElectroMagnetic Analysis (DEMA). Application of SCA on cryptographic primitives and recent advances with application of ML is previously discussed in [58, 59].

When a given implementation of ML model is executed, it generates a physical signature. This physical signature, although unintentional, exists in various forms. Let us take an example of execution time of a neural network. This execution time will depend on structural parameters of the network like the number of layers, number of neurons, etc. Even inside a single neuron, the choice of the activation function influences the execution time, as shown in [5]. Activation functions like ReLU are a lot less resource intensive compared to sigmoid or tanh that involve complex operations like exponentiation. Moreover, the input-dependent execution time of exponentiation can reveal information about the input to an adversary who has access to detailed timing patterns. Similar vulnerabilities can also be exploited by an adversary who has access to other physical signatures.

Those side channels have been demonstrated to leak sensitive parameters of a ML model like number of layers, number of neurons, activation functions, secret weights, filter size etc. The leakage can be exploited in several scenarios. We identified three attack scenarios presented in the literature, which are as follows:

Model Extraction: The adversary aims to recover parameters of the target model with as much precision as possible. Such attacks are relevant for the IP theft scenario where adversary has cost benefits to recover secret black box model and evade payment for additional licences. Precise knowledge of the model can also be exploited to learn information on training data, which can be sensitive and critical in certain settings like healthcare.

Substitute Model Extraction: This is a weaker version of the model extraction attack where an adversary aims at recovering a model that performs similarly to the target black box model. The performance can be expressed in metrics like testing accuracy.

Input Recovery: For certain applications, where the input to a model is privacy sensitive, appropriate security measures like encryption are used to not communicate the sensitive input in plaintext over an open communication channel. However, for inference tasks, the input must be decrypted before any processing by the ML model. Side-channel leakage of interactions between the model and the secret input can also be exploited to learn information about the input. Most, if not all attacks in this class exploit the processing of the input in the first layer where processing is done on raw inputs.

2.1 State-of-the-Art
Previous works demonstrating vulnerabilities of ML models against side-channel attacks have looked into the following physical quantities:

Timing Side Channel: The vulnerabilities under this class exploit the fact that the time of internal computation relates to the parameters of the target model. Previous research has shown that both, execution time and the time to access the model parameters can leak critical information.

Power/EM Side Channel: Power consumption or EM emanations of internal operations can be exploited to gain information on involved sensitive values. Vulnerabilities in this class typically target, but are not limited to, basic operations like weight multiplication or CONV filters.

Microarchitectural Side Channel: Executing ML models on commodity hardware like high-end CPU or GPU results in leakage at the microarchitectural level. Most known works have exploited cache access patterns in terms of timing related to cache hit or cache miss.




Timing Side-Channel. One of the earliest works on reverse engineering targeting Convolution Neural Networks (CNN) with Side-Channel Attacks (SCA) was proposed by Hua et al. [27]. The work acknowledges the sensitivity of the model and assumes standard protection like executing model computation in secure enclaves like Intel Software Guard Extensions (SGX). It exploits information leakage through timing (and memory) side-channels targeting CNN accelerators running in a secure enclave. The secure enclave prevents an adversary from accessing information of the execution, however, the off-chip memory accesses are still observable, which eventually allows reverse engineering of CNN structure and weights. Owing to the huge size of CNN models, it is not possible to store all weights/parameters on the on-chip memory, rather off-chip memory stores the parameters and is accessed when required in the computation. Memory access patterns reveal information of accessed memory locations and read/write patterns. The information leakage on access patterns remains available even if the memory is encrypted. As a result, Hua et al. demonstrate the retrieval of key network parameters like the number of layers, input/output sizes of each layer, size of filters, data dependencies among layers, etc. This allows an attacker to infer a small set of possible network structures by exploiting the execution time of a computation on a CNN accelerator.

Two commonly popular networks were successfully targeted, AlexNet [33] and SqueezeNet [28]. Once the network structure is recovered, the proposed attack can be extended to reverse engineer the secret weights of the CNN. The attack assumes usage of dynamic zero pruning in the CNN architecture and exploits it for weight recovery, with knowledge of the inputs. However, the exploited memory access patterns are available under a strong assumption like hardware Trojan, physical memory bus probing or compromised OS. Activation functions like ReLU which converts any negative input to zero, result in a large number of zeros in intermediate results of an inference. These extra zeros can be pruned to optimize storage. However, this memory optimization leaks the number of zero-valued pixels pruned by the activation function, which can then be exploited to retrieve information on weight and bias, in particular their ratio. The knowledge of ratio significantly reduces the entropy of weights. The authors demonstrated the recovery of the weight and the bias for the first layer of AlexNet. Weight recovery attacks on deeper layers were not investigated.

Duddu et al. [17] proposed a timing based attack on neural networks. The attack can target different hardware architectures or dedicated accelerators as long as the victim and the adversary use copies of the same hardware. The timing information under the black box model is used to determine the network depth under a fixed number of queries. With the information on network depth, the authors generate a substitute model of comparable accuracy to the original model. Reinforcement learning is used to reconstruct a substitute architecture. The attack is demonstrated on VGG [46]-like deep architectures on an Intel Xeon Gold 5115 platform, and the authors argue that the proposed method can be extended to any other hardware accelerator as well. The test accuracy of the substitute model is within 5% error margin from the targeted model architecture. Note that this method does not allow model extraction but finds a substitute network.

Power and EM Side-Channel. Batina et al. [4] proposed a full reverse engineering of neural network parameters based on power/EM side-channel analysis. The proposed attack is able to recover key parameters i.e., activation function, pre-trained weights, number of hidden layers and neurons in each layer, without access to any training data. The adversary uses a combination of simple power/EM analysis, differential power/EM analysis and timing analysis to recover different parameters. In the following, we describe in brief how different parameters are recovered.

The attack targets a trained model of a feed-forward neural network deployed on an embedded device for testing. The adversary feeds known random inputs in a form of floating point real numbers and observes side channels. Fixed point numbers make the attack easier. The measurement setup is shown in Fig. 1(a). A sample EM trace is shown in Fig. 1(b) for a 4-layer Multilayer Perceptron (MLP) with (50, 30, 20, 50) neurons. One can easily distinguish each of the 4-layers. Moreover, it is shown in [4], that the adversary can zoom into each neuron and observe each multiplication and activation function. In other words, the adversary can collect a large amount of traces in one go and reuse individual parts of the traces for recovering different parameters of the network.



The first step is to recover the activation function for each neuron. The activation function is a non-linear component in the neural network processing and normalizing its complex implementation results in a non-constant time execution. Note that the adversary does not need new timing measurements. The EM traces provides precise timing patterns for each activation function in each neuron. Table 2 shows minimum, maximum and mean execution times of sigmoid, tanh and ReLU activation functions which can be matched to a pre-characterized profile for recovery. The pre-characterized timing profile of the activation functions, when compared with unlabeled timing profile of target activation function, will reveal the function with high probability. Even though some functions may have similar timing profiles like sigmoid and tanh, still with enough test samples, the two functions are easily distinguishable from each other owing to different mean timing and corresponding ranges (see Table 2).










The next step is to recover the individual weights. The weights are recovered using differential power/EM analysis and the Pearson correlation coefficient is used as a statistical distinguisher. The attack targets the multiplication 
 of a known input x with a secret weight w. The leakage model for the used embedded microcontroller is the Hamming weight (HW). The adversary makes hypothesis on the weight and correlates the activity of the predicted output m with measured traces t. The correct value of the weight w will show higher correlation compared to all other wrong hypotheses 
, given enough measurements. The attack was demonstrated in real numbers in IEEE 754 format, where 32-bit representation is used. To keep the number of hypothesis in check, the attack is performed byte-wise and the 32-bit weight is recovered in 4 parts. It is also observed, that unlike in cryptography, exact weights are not required and some precision errors in recovery can be tolerated without affecting the accuracy of the network. DEMA can also be further used to determine layer boundaries, when not possible. Given an input to the network, the correlation of weight multiplications will be much higher in the first layer as compared to subsequent layers, thus allowing distinguishing neurons belonging to the first layer.

The full network is recovered in an iterative manner with a combination of these developed techniques. The network is recovered from input to output, neuron by neuron and layer by layer. The attack scales linearly with the size of the network and the same set of traces can be reused for various steps of the attack limiting the measurement effort.

Dubey et al. [16] proposed a power-based side-channel attack on a Binarized Neural Networks (BNN) to recover secret parameters such as weights and biases. In contrast to Batina et al. [4], the target platform is parallel hardware accelerators running a 7-series FPGA board mounted on a SAKURA-X board. They exploit power leakage and perform a basic correlation attack on 4-bit of the weights and demonstrate a successful weight recovery with 200 measurements only. Authors further propose design of BNN accelerators that can resist DPA using countermeasures like masking.

Yu et al. [56] proposed a model extraction attack based on combination of EM side-channel measurement and adversarial active learning to recover the Binarized Neural Networks (BNNs) architecture on popular large-scale NN hardware accelerators. The network architecture is first inferred through EM leakage, and then, the parameters are estimated with adversarial learning. For the layer topology reverse engineering, the attacker observes the average timing behavior from the EM traces. This is based on the observation that different layers will result in different execution times. For example, the pooling layer typically requires a shorter time than a convolution one, and a fully-connected layer is observed to have the longest execution time, since it requires most of the sequential XNOR computations. Thus, by observing the timing profile, the adversary could reconstruct the network architecture. In the adversarial learning setting, the attacker crafts malicious inputs for the query, which could be used to identify the decision boundary for the trained model. For the attack, the adversary is assumed to be incapable of accessing the training data or knowing the model. The attack shown through the experiment could recover 96–99% in comparison to the black box model.

Considering input recovery attack using power/EM side-channel, the first attack was reported by Wei et al. [53]. Authors demonstrate recovery of the input image from FPGA based CNN accelerator. The proposed attack exploits a specific design choice, i.e., the line buffer in a convolution layer of a CNN. Two attack scenarios were presented considering different adversarial capabilities. The first one is the passive model, where the adversary eavesdrops the power consumption during the execution. Assuming that if the processed data is unchanged between cycles, the internal transitions will be limited, resulting in lower power consumption, and thus, by monitoring the power leakage, the adversary could determine if the pixels share similar values, whether they belong to the background of the image. The other is the active model, where the adversary is profiling the correlation between power signals, by building a power template. The power template characterizes the mapping between pixel values and the corresponding power leakage, under different kernels. The experiment conducted on a MNIST dataset reported a recovery success of 
.

Batina et al. [5] also reported an input recovery attack on embedded platforms. They consider known or commonly used networks where the weights are either public or independently recovered using one of the reverse engineering techniques. Only the first layer weights are crucial for this attack which targets the multiplication between secret input and known weights. This attack is similar to the previous attack on multiplication proposed in [4]. The attack targets the multiplication 
 of a secret input x with a known weight w. The issue in this case is that each input is only processed once and thus must be recovered in a single measurement. To overcome this limitation, an adversary can exploit individual weight multiplications in different neurons, captured on different part of the same trace. Thus, EM measurements corresponding to a fixed unknown input and several known weights are present in the same measurement, which can be broken into short independent traces to conduct a classical correlation-based attack. This kind of attacks which exploit different computations in the same measurements are popularly known as horizontal attacks. For bigger networks with a large input layer, the amount of individual multiplications available to an adversary increases, thus allowing a bigger measurement set to perform the attack. The recovery was shown on MNIST images with a precision error of 2 decimal places resulting in almost no visual differences between original and recovered images. The attack also applies on CNN, where a single input value might be processed several times (due to convolution operation). A similar vulnerability as shown in [5] was exploited through timing side-channel by Dong et al. [15] to recover input MNIST images with 96% accuracy on 4-layer MLP running on an 8-bit Atmel XMEGA128 microprocessor. They exploit the fact that input multiplication with constant weights will result in a variable time floating point multiplication. The precise timing of the multiplication can be recovered by observing power side-channel trace.

Microarchitectural Side-Channel. Recently, some works based on microarchitectural attacks have also been proposed for the reverse engineering of Deep Learning (DL). Yan et al. [54] have proposed the Cache Telepathy method. The observation is that, for typical NN , the multiplication operation depends on GEMM (Generalized Matrix Multiply). In this case, the architecture parameters of the network will determine how many times the GEMM is called or the dimension of the matrices, which can be revealed through cache side-channel. The attack is based on common cache-based SCA, Flush+Reload [55] and Prime+Probe [36]. The target networks are VGG-16 [46] and ResNet-50 [22]. Using the proposed method, the search space for the architecture can be significantly reduced. In this work, the attack could reveal matrix multiplication related parameter such as convolutional or fully connected layers, and for others such as activation and pooling layers, it might be harder to recover.

Similarly, the authors of [24] proposed DeepRecon, an attack methodology based on cache side-channel that exploits Flush+Reload to reconstruct the Deep Neural Network (DNN) architecture. In their attack model, rather than accessing the target model directly like in other side-channel based attack, the adversary runs a co-located process on the host machine, in which the victim’s model is also running. Similar to earlier work [54], the proposed attack does not generalize to computations on hardware other than a CPU. Also, they find that based on how the matrix multiplication is implemented, they are unable to estimate the inputs and parameters of a victim’s model. As such, they hypothesize that this might be the limit of cache-based SCA on DNN.

Hu et al. [26] proposed an attack targeting GPU platform and highlighted some of the potential issues arising in contrast to other works. The attack is using the bus snooping technique, exploiting the off-chip memory address traces and PCIe events. The idea used in this work is that inter-layer DNN architecture features will be considered as string of “sentences”, so by considering this instead of individual “word”, it might maximize the likelihood of a correct match far more effectively than character-by-character approaches. To perform the experiment, they consider the Long short-term memory (LSTM) model, a common neural network, with CTC (Connectionist Temporal Classification) decoder, which is commonly used in Automatic Speech Recognition. The attack only requires the assumption that the adversary can observe the architectural side-channel over time. It also assumes the adversary can feed specific input and observe the results. The experiments are conducted on off-the-shelf Nvidia GPU running CNN, in a parallel manner, and the victim’s model is ResNet-18 [22].

2.2 Countermeasures
The success of SCA on ML models can be mainly attributed to the naïve implementation of the model. Previously, ML models were rarely seen in hostile environments with adversaries benefiting from physical access. However, with IoT and edge-based devices, the threats have become real as highlighted by the range of works mentioned above. Thus, countermeasures must be investigated. As such, there is a wide research on SCA countermeasures against cryptographic implementations which can be also applied on ML models. However, direct application of countermeasures would result in a non-negligible overhead. In the following, we discuss some directions for countermeasures considering different physical side-channels.

Timing Side-Channel. It has been shown by multiple works that the execution time depends on network parameters which eventually leak sensitive information to the adversary. To overcome this problem, the designer can take two approaches. The first approach is to have constant time implementations of basic functions [43]. This can solve some issues where the value of the input is determined from the execution time but other issues like distinguishing between components (e.g. sigmoid vs. ReLU) may still be possible. The other approach is to randomize the execution timing in a way that it becomes independent of the sensitive information executed preventing an adversary to learn by observing timing information. This would require access to a good source of randomness and techniques like jitter and dummy operations can be used [38].

Power/EM Side-Channel. Hiding and Masking are the two typical types of countermeasures used against power/EM side-channel. Hiding aims at reducing the signal to noise ratio in a measurement, making attacks difficult. Masking uses randomization by mixing computation with random data to remove any correlation between sensitive variables and power/EM signature. Dubey et al. [16] proposed the first countermeasure for DNN against SCA. The countermeasure, referred to as MaskedNet, is based on masking. The resulting design uses novel masked components such as masked adder trees for fully-connected layers and masked Rectifier Linear Units for activation functions. They even use hiding countermeasure, like Wave Differential Dynamic Logic (WDDL) [49], to protect the sign-bit computation. The proposed protection increases the latency and area-cost by 2.8 and 2.3 times, respectively. When tested against first-order DPA, the attack against masking fails even when using 100k traces, however second-order DPA on masking can still break it with just 3.7 k traces. When analyzing the Difference-of-Means (DoM) test on the sign-bit computation, after 40k of traces, bit 0 and bit 1 can be distinguished. The argument for this is in the low noise platform used.

Microarchitectural Side-Channel. Much like timing and power/EM side-channel, hiding or randomizing cache activity of ML model execution can prevent such attacks. However, unsurprisingly, any of those choices result in performance overheads. Alam and Mukhopadhyay [3] proposed a countermeasure against microarchitectural side-channel attacks on ML models by observing the Hardware Performance Counter (HPC). During the execution of CNN, an evaluator, who does not know the detail of the implementation but can monitor various HPC events, conducts statistical hypothesis testing on the distribution of the data that can detect an attack. The evaluator throws an alarm when there is an anomaly in the distribution signifying potential side-channel leakage.

Thus, as a general observation, ML models do suffer from side-channel vulnerabilities and existing countermeasures stem from either hiding or masking families of countermeasures. In practice, a combination of various countermeasures is more likely deployed. However, for modern architectures, the network architecture can easily grow to millions of parameters, and such, the countermeasure overhead might make it impractical to implement. Thus, ML friendly countermeasures in terms of the overhead in cost must be investigated.







3.3 Countermeasures
Fault injection countermeasures can be deployed at different levels of the design – model architecture, software implementation, and hardware layer. In this part, we will discuss each of these in more detail.

Model Architecture. Neural networks contain vast amount of interconnected nodes. Because of their working principle, not all of them are activated for every input. Therefore, if faults are injected into nodes that are unused in the current execution, there will not be any outcome [41]. This behavior is known as partial fault tolerance and it was shown that neural network implementations of cryptographic operations can make them more resistant to faults than standard implementations [2]. The more redundancy is in the network, the better fault tolerance can be achieved, at the cost of higher memory usage and computation complexity.

Software Implementation. Redundancy and checks can be added in the model computation on the software level. A naïve approach would be to repeat the computation two or more times and then compare the results. If they are not equal, the device might have been tampered with. Redundant instruction sequences can protect against pre-defined number of faults [39]. Data within the instructions can be arranged in a redundant way that will protect against both data corruption and instruction skips [42]. Non-linear codes can be used to implement the operations that allow protection against multiple bit faults per operation [7].

Hardware Layer. Error detection and correction codes can be efficiently implemented in hardware. Computational circuits, or parts of them can be implemented in parallel and majority voting can be utilized to prevent outputting the faulty result [10]. Additional circuits can be deployed to detect voltage variations caused by fault injection. These circuits can raise an alarm and a pre-defined action to prevent the information leakage or release of incorrect output can be taken [23].

Additionally, there are physical measures that can be applied to prevent tampering, such as special shielding of the chip or erasing of memory if the chip package is damaged.

4 Conclusion
We have surveyed known physical attacks used for the purpose of reverse engineering ML models on a range of platforms and discussed possible countermeasures. The results published so far demonstrate that stealing the models in this way (as possible IP) is a clear and present threat. Specific use cases and applications on various edge and IoT devices should be carefully examined against those threats and accordingly protected.

5 Open Research Problems
Powerful adversaries today include those exploiting side-channel leakage from implementations on ML models and the ability to actively disturb the device’s operations. Combining the two poses even more challenge to the engineering efforts in designing adequate defenses. The countermeasures considered so far are mainly from the crypto/security applications, which makes them sub-optimal. On top, typical overheads in resources such as power/energy makes those defenses often unsuitable for low-end devices. As the adversaries are becoming ever more powerful and knowledgeable, it is necessary to revisit the design cycle and make it open at various phases such that the results of preliminary security evaluation can still be fed back to the implementations.

We see future works going more into directions of ML-specific countermeasures and new frameworks to evaluate the leakages before the models are put into the field.